
Link : https://drive.google.com/drive/u/0/folders/1oxqE6vIbbggmgZESC5sqPeXKPyEOxVcB

Aim:
Implementation of learning algorithms for an application

DESCRIPTION:
Here, we implement a Decision Tree to determine if a product has enough weight and is
pushed to
the right (R) or is of insufficient weight and is pushed to the left (L). The Dataset used is the
Balance
Scale Weight & Distance Database.

Algorithm:
Entropy is the measure of uncertainty of a random variable, it characterizes the
impurity of an
arbitrary collection of examples. The higher the entropy the more the information
content.
1. Start
2. Consider the whole training set as the root
3. Attributes are assumed to be categorical and are distributed recursively according
to values using
statistical methods
4. Find the best attribute and place it on the root node of the tree.
5. Now, split the training set of the dataset into subsets. While making the subset
make sure that

each subset of the training dataset has the same value for an attribute. The entropy
increases for
each partition.
6. Find leaf nodes in all branches by repeating steps 1 and 2 on each subset
7. End

Source Code:
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
def splitdataset(balance_data):
X = balance_data.values[:, 1:5]
Y = balance_data.values[:, 0]
X_train, X_test, y_train, y_test = train_test_split(
X, Y, test_size = 0.3, random_state = 100)
return X, Y, X_train, X_test, y_train, y_test
def prediction(X_test, clf_object):
y_pred = clf_object.predict(X_test)
print("Predicted values:")

print(y_pred)
return y_pred
if name ==" main ":
data =

pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/balance-
scale/balance-
scale.data', sep= ',', header = None)

X = data.values[:,
1:5]Y =
data.values[:, 0]
X_train, X_test, y_train, y_test = train_test_split(X, Y,
test_size =0.3, random_state = 100)
clf_entropy = DecisionTreeClassifier(criterion = "entropy", random_state
= 100,max_depth = 3, min_samples_leaf =
5)clf_entropy.fit(X_train, y_train)
y_pred = prediction(X_test, clf_entropy)
print("Accuracy : ",
accuracy_score(y_test,y_pred)*100)

Output:

Result:
Implementation of learning algorithms using Python is carried out successfully
